{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras Tuner\n",
    "**Hyperparameters** are the variables that govern the training process and the topology of an ML model. These remain constant over the training process and directly impact the performance of the ML program. \n",
    "\n",
    "The process of finding the optimal set of hyperparameters is called *hyperparameter tuning* or *hypertuning*, and it is an essential part of a machine learning pipeline.\n",
    "\n",
    "Hyperparameters are of two types:\n",
    "1. *Model hyperparameters* which influence model selection such as the number and width of hidden layers\n",
    "\n",
    "2. *Algorithm hyperparameters* which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier.\n",
    "\n",
    "For more complex models, the number of hyperparameters can increase dramatically and tuning them manually can be quite challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "26435584/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "4431872/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download the Fasion MNIST dataset\n",
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preprocessing, normalize the pixel values to make the training converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance\n",
    "\n",
    "First, find a baseline performance using arbitrarily handpicked parameters to compare the results later.\n",
    "\n",
    "We will be building a shallow **Dense Neural Network (DNN)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               401920    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the baseline model using the Sequential API\n",
    "b_model = keras.Sequential()\n",
    "b_model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "b_model.add(keras.layers.Dense(units=512, activation='relu', name='dense_1')) # Will tune this layer later\n",
    "b_model.add(keras.layers.Dropout(0.2))\n",
    "b_model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Print model summary\n",
    "b_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, we hardcoded all the hyperparameters when declaring the layers. These include the number of hidden units, activation, and dropout.\n",
    "\n",
    "Let's then setup the loss, metrics, and the optimizer. The learning rate is also a hyperparameter that can be tuned automatically but for now, let's set it at `0.001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training parameters\n",
    "b_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.5141 - accuracy: 0.8170 - val_loss: 0.4076 - val_accuracy: 0.8525\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3924 - accuracy: 0.8564 - val_loss: 0.3855 - val_accuracy: 0.8635\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3559 - accuracy: 0.8692 - val_loss: 0.3494 - val_accuracy: 0.8743\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3325 - accuracy: 0.8780 - val_loss: 0.3403 - val_accuracy: 0.8802\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3183 - accuracy: 0.8816 - val_loss: 0.3474 - val_accuracy: 0.8715\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3043 - accuracy: 0.8855 - val_loss: 0.3242 - val_accuracy: 0.8813\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2891 - accuracy: 0.8920 - val_loss: 0.3260 - val_accuracy: 0.8847\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2811 - accuracy: 0.8945 - val_loss: 0.3364 - val_accuracy: 0.8848\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2732 - accuracy: 0.8970 - val_loss: 0.3204 - val_accuracy: 0.8845\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2662 - accuracy: 0.9008 - val_loss: 0.3208 - val_accuracy: 0.8862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1677c2090>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training epochs.\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Train the model\n",
    "b_model.fit(img_train, label_train, epochs=NUM_EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3434 - accuracy: 0.8791\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on the test set\n",
    "b_eval_dict = b_model.evaluate(img_test, label_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function for displaying the results so it's easier to compare later. Prints the values of the hyparameters to tune, and the results of model evaluation\n",
    "- model (Model) - Keras model to evaluate\n",
    "- model_name (string) - arbitrary string to be used in identifying the model\n",
    "- eval_dict (dict) -  results of model.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BASELINE MODEL:\n",
      "number of units in 1st Dense layer: 512\n",
      "learning rate for the optimizer: 0.0010000000474974513\n",
      "loss: 0.34340524673461914\n",
      "accuracy: 0.8791000247001648\n"
     ]
    }
   ],
   "source": [
    "def print_results(model, model_name, eval_dict):\n",
    "\n",
    "    print(f'\\n{model_name}:')\n",
    "\n",
    "    print(f'number of units in 1st Dense layer: {model.get_layer(\"dense_1\").units}')\n",
    "    print(f'learning rate for the optimizer: {model.optimizer.lr.numpy()}')\n",
    "\n",
    "    for key,value in eval_dict.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "# Print results for baseline model\n",
    "print_results(b_model, 'BASELINE MODEL', b_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tuner\n",
    "To perform hypertuning with Keras Tuner, we need to:\n",
    "\n",
    "* Define the model\n",
    "* Select which hyperparameters to tune\n",
    "* Define its search space\n",
    "* Define the search strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/Users/minjaegil/miniconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/Users/minjaegil/miniconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/Users/minjaegil/miniconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/Users/minjaegil/miniconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/Users/minjaegil/miniconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rpcio (/Users/minjaegil/miniconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/minjaegil/miniconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Keras Tuner\n",
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minjaegil/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model\n",
    "The model we set up for hypertuning is called a *hypermodel*. When we build this model, we define the hyperparameter search space in addition to the model architecture. \n",
    "\n",
    "We can define a hypermodel through two approaches:\n",
    "\n",
    "* By using a model builder function\n",
    "* By [subclassing the `HyperModel` class](https://keras-team.github.io/keras-tuner/#you-can-use-a-hypermodel-subclass-instead-of-a-model-building-function) of the Keras Tuner API\n",
    "\n",
    "Here, we will be using the first approach; we will use a model builder function to define the image classification model. This function returns a compiled model and uses hyperparameters you define inline to hypertune the model. \n",
    "\n",
    "The function below basically builds the same model we used earlier. The difference is that there are two hyperparameters that are setup for tuning:\n",
    "\n",
    "* the number of hidden units of the first Dense layer\n",
    "* the learning rate of the Adam optimizer\n",
    "\n",
    "For this practice, we will: \n",
    "\n",
    "* use its `Int()` method to define the search space for the Dense units. This allows you to set a minimum and maximum value, as well as the step size when incrementing between these values. \n",
    "\n",
    "* use its `Choice()` method for the learning rate. This allows you to define discrete values to include in the search space when hypertuning.\n",
    "\n",
    "All available methods and its sample usage can be found in the [official documentation](https://keras-team.github.io/keras-tuner/documentation/hyperparameters/#hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the model and sets up the hyperparameters to tune.\n",
    "# hp - Keras tuner object\n",
    "# returns model with hyperparameters to tune\n",
    "def model_builder(hp):\n",
    "\n",
    "  # Initialize the Sequential API and start stacking the layers\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu', name='dense_1'))\n",
    "\n",
    "  # Add next layers\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Tuner and Perform Hypertuning\n",
    "Now that we have the model builder, we can then define how the tuner can find the optimal set of hyperparameters, also called the **search strategy**. Keras Tuner has [four tuners](https://keras-team.github.io/keras-tuner/documentation/tuners/) available with built-in strategies - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. \n",
    "\n",
    "Here, we will use the **Hyperband tuner**. Hyperband is an algorithm specifically developed for hyperparameter optimization. It uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket wherein the algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Intuition behind the algorithm can be found in section 3 of [this paper](https://arxiv.org/pdf/1603.06560.pdf).\n",
    "\n",
    "Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer.\n",
    "\n",
    "* the hypermodel (built by our model builder function)\n",
    "* the `objective` to optimize (e.g. validation accuracy)\n",
    "* a `directory` to save logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner.\n",
    "* the `project_name` to differentiate with other runs. This will be used as a subdirectory name under the `directory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the tuner\n",
    "tuner = kt.Hyperband(model_builder, \n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='kt_dir',\n",
    "                     project_name='kt_hyperband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "# Display hypertuning settings/summary\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass in a callback to stop training early when a metric is not improving. Below, we define an [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback to monitor the validation loss and stop training if it's not improving after 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above. This will take around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 52s]\n",
      "val_accuracy: 0.8493333458900452\n",
      "\n",
      "Best val_accuracy So Far: 0.8899166584014893\n",
      "Total elapsed time: 00h 07m 43s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Perform hypertuning\n",
    "tuner.search(img_train, label_train, epochs=NUM_EPOCHS,\n",
    "             validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the optimal hyperparameters from the results using [get_best_hyperparameters()](https://keras-team.github.io/keras-tuner/documentation/tuners/#get_best_hyperparameters-method)  method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first \n",
      "densely-connected layer is 192 and the optimal learning rate \n",
      "for the optimizer is 0.001.\n"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first \n",
    "densely-connected layer is {best_hps.get('units')} and the optimal learning rate \n",
    "for the optimizer is {best_hps.get('learning_rate')}.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model\n",
    "Now that we have the best set of hyperparameters, we can rebuild the hypermodel with these values and retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 192)               150720    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1930      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 152,650\n",
      "Trainable params: 152,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "h_model = tuner.hypermodel.build(best_hps)\n",
    "h_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.5387 - accuracy: 0.8089 - val_loss: 0.4268 - val_accuracy: 0.8438\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4036 - accuracy: 0.8532 - val_loss: 0.3855 - val_accuracy: 0.8611\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3659 - accuracy: 0.8673 - val_loss: 0.3641 - val_accuracy: 0.8700\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3471 - accuracy: 0.8725 - val_loss: 0.3633 - val_accuracy: 0.8679\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3285 - accuracy: 0.8784 - val_loss: 0.3357 - val_accuracy: 0.8790\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3144 - accuracy: 0.8845 - val_loss: 0.3271 - val_accuracy: 0.8827\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3032 - accuracy: 0.8862 - val_loss: 0.3312 - val_accuracy: 0.8813\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2919 - accuracy: 0.8921 - val_loss: 0.3363 - val_accuracy: 0.8814\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2835 - accuracy: 0.8938 - val_loss: 0.3295 - val_accuracy: 0.8816\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2763 - accuracy: 0.8968 - val_loss: 0.3235 - val_accuracy: 0.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa157622590>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the hypertuned model\n",
    "h_model.fit(img_train, label_train, epochs=NUM_EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3559 - accuracy: 0.8768\n"
     ]
    }
   ],
   "source": [
    "h_eval_dict = h_model.evaluate(img_test, label_test, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BASELINE MODEL:\n",
      "number of units in 1st Dense layer: 512\n",
      "learning rate for the optimizer: 0.0010000000474974513\n",
      "loss: 0.34340524673461914\n",
      "accuracy: 0.8791000247001648\n",
      "\n",
      "HYPERTUNED MODEL:\n",
      "number of units in 1st Dense layer: 192\n",
      "learning rate for the optimizer: 0.0010000000474974513\n",
      "loss: 0.35585683584213257\n",
      "accuracy: 0.876800000667572\n"
     ]
    }
   ],
   "source": [
    "# Print results of the baseline and hypertuned model\n",
    "print_results(b_model, 'BASELINE MODEL', b_eval_dict)\n",
    "print_results(h_model, 'HYPERTUNED MODEL', h_eval_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have **reduced the model size** (decrease in units) and saved compute resources while still having more/less/same accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official document: [Keras Tuner Reference](https://keras.io/guides/keras_tuner/getting_started/#the-search-space-may-contain-conditional-hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
